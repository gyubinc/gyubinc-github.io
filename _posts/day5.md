---
layout: post
title: "Day 5"
---

# 5) 딥러닝 학습방법
---
**신경망**

선형모델과 비선형함수의 결합

**선형모델**

O(n x p) = XW + b

* X(데이터 행렬, n x d)

* W(가중치행렬, d x p)

* b(절편, n x p)

**softmax**

분류문제 해결에 이용하는 연산자

* 확률벡터 형태로 클래스 분류

* 학습할 때 사용

* 추론할 떄는 one-hot vector 사용

**numpy overflow**

numpy는 너무 큰 값을 받을 때 overflow가 발생하므로 np.max를 활용해 해결

**활성화함수**

신경망은 선형모델과 활성함수(activation function)를 합성한 함수

* 자신의 노드에만 관여(softmax와는 다름)

* 활성화함수가 없다면 선형모델임

* sigmoid 함수

* tanh 함수

* ReLU 함수 (max{0, x})

**다층 신경망**

multi-layer perceptron, 신경망이 여러층 합성된 함수

* 가중치 행렬의 수가 층수

**forward propagation**

순전파

* 순차적인 계산을 통해 계산하는 과정

* 입력이 들어오면 출력을 내뱉는 과정

**층이 여러개인 이유**

* 이론적으로는 2층 신경망으로도 근사 가능

* 층이 깊을수록 필요한 뉴런의 숫자가 줄어듦

* 층이 얇으면 너무 넓은 신경망이 됨

**back propagation**

역전파

* 손실함수에 대한 gradient값을 통해 전파

* 연쇄법칙을 통해 역순으로 순차적으로 계산

* 모든 노드에 대해 계산

**자동미분**

합성함수 미분법인 연쇄법칙 기반 auto-differentiation

* 각 노드의 텐서 값을 컴퓨터가 기억해야 미분 계산 가능

* 순전파보다 메모리를 많이 차지한다

# 6)확률론 맛보기
---

딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있다.

* loss function은 데이터 공간을 통계적으로 해석해서 유도함

* 회귀 loss function : L2-Norm = 예측 오차의 분산을 가장 최소화하는 방향으로 학습

* 분류 loss function : cross-entropy = 모델 예측의 불확실성 최소화하는 방향으로 학습


**확률 분포는 데이터의 초상화**

**가정**

데이터 공간을 **X** x y로 상정(데이터 **X**와 정답레이블 y, 지도학습)

*D*는 데이터 공간에서 데이터를 추출하는 분포

확률변수 (**X**,y) ~ *D*로 표기

**확률변수**

* 이산(discrete) 확률변수

* 연속(continuous) 확률변수

확률 변수는 데이터공간이 아닌 분포 *D*에 의해서 결정된다

* 컴퓨터는 이산형, 연속형 확률변수 모두 다룸

**이산형 확률변수**

확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링

**연속형 확률변수**

데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링

**결합분모**

결합분포(joint dirtribution) P(**x**,y) 는 *D*를 모델링

* 원래 확률분포에 상관없이 이산, 연속 모델링 선택가능

**marginal distribution**

주변확률 분포 P(**x**)는 입력 **x**에 대한 주변확률분포로 y에 대한 정보를 주진 않음

* 결합확률 분포를 각각의 y에 대해 더해줘서 계산

**조건부 확률분포**

P(**X**|y)는 입력 **X**와 출력 y 사이의 관계를 모델링

* 특정 클래스가 주어진 조건에서 데이터의 확률분포

<br/>

**조건부확률과 기계학습**

조건부 확률 P(y|**x**)는 입력변수 x에 대해 정답이 y일 확률을 의미

* 연속확률분포의 경우 확률이 아니고 밀도임

* 로지스틱 회귀에서 사용했던 선형모델과 softmax함수의 결합은 데이터에서 추출된 패터늘 기반으로 확률 해석에 사용

* 분류 : softmax(W phi + b)는 phi(x)와 가중치행렬 W를 통해 조건부확률 계산

* 회귀 : 조건부기대값 E[y | **x**] 추정

**기대값**

확률분포가 주어지면 데이터 분석에 사용하는 통계적 범함수(statistical functional)를 계산가능

* 기대값은 데이터를 대표하는 통계량

* 기대값을 이용해 분산, 첨도, 공분산 계산 가능

**딥러닝은 다층신경망을 사용하여 데이터로부터 특징 패턴을 추출**

**몬테카를로 샘플링**

데이터를 이용하여 기대값을 계산할 때 사용하는 샘플링 방법

* 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때임

* 몬테카를로는 이산형이든 연속형이든 상관없이 성립

* 독립추출이 보장되어야 함

* 대수의 법칙(law of large number)에 의해 수렴성 보장

# 7)통계학 맛보기
---

**통계적 모델링**

적절한 가정 위에서 확률분포를 추정(inference)하는 것

* 근사적으로 확률분포 추정

* 정확히 맞추는 것은 불가능하므로 예측의 위험성 최소화 방향

**모수적 방법론**

특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 경정하는 모수(parameter)를 추정하는 방법

**비모수적 방법론**

특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀜

* 기계학습의 많은 방법론은 비모수적

**확률분포 가정**

* 데이터를 생성하는 원리를 먼저 고려하는 것이 원칙

* 베르누이 분포 : 데이터가 2개의 값

* 카테고리 분포 : 데이터가 n개의 이산적인 값

* 베타분포 : 데이터가 [0,1] 사이

* 감마분포, 로그정규분포 : 데이터가 0 이상의 값

* 정규분포, 라플라스분포 : 데이터가 실수 전체의 값

* 모수 추정 후 통계적 검정 필수

**표집분포**

sampling distribution, 통계량의 확률분포

* 표본평균, 표본분산의 분포

* 표본평균의 경우 N이 커질수록 정규분포를 따른다

* 중심극한정리 (Central Limit Theorem)

**최대가능도 추정법**

이론적으로 가장 가능성이 높은 모수를 추정하는 방법

* maximum likelihood estimation, MLE

* 데이터 집합 **X**가 독립적으로 추출되었을 경우 로그가능도를 최적화

**왜 로그가능도를 사용할까**

* 로그가능도를 최적화하는 모수는 가능도를 최적화하는 MLE가 됨

* 데이터가 매우 커지면 가능도를 계산하는 것이 불가능

* 독립일 경우, 로그를 사용해 가능도의 곱셈을 덧셈으로 변경

* 경사하강법으로 가능도를 최적화할 때 미분 연산량이 O(n^2)에서 O(n)으로 감소

* 손실함수는 보통 경사하강법을 사용하므로 음의 로그가능도(negative log-likelihood) 최적화

**MLE는 불편추정량을 보장하진 않는다**

**딥러닝에서 최대가능도 추정법**

* 최대가능도 추정법을 이용해 기계학습모델 학습

* 딥러닝 모델의 가중치 (W1, W2 .. WL)에 대해 분류문제에서 softmax vector는 카테고리 분포의 모수 (p1, p2.. pk) 모델링

* 원핫벡터로 표현한 정답레이블(y1, ..., yk)을 관찰데이터로 이용해 확률분포인 softmax vector의 log-likelihood 최적화

**확률분포의 거리**

기계학습의 손실함수들은 모델이 학습하는 데이터에서 관찰되는 확률분포의 거리를 통해 유도

* 데이터 공간에 두 개의 확률분포 P(x), Q(x)가 있을 때 확률분포 사이의 거리

* 총변동 거리(Total Variation Distance, TV)

* 쿨백-라이블러 발산(Kullback-Leibler Divergence, KL)

* 바슈타인 거리(Wasserstein Distance)

**쿨백-라이블러 발산**

KL Divergence

* 어떤 이상적인 분포에 대해 그 분포를 근사하는 다른 분포를 사용해 샘플링한다면 발생할 수 있는 정보 엔트로피 차이를 계산

* 분류 문제에서 정답레이블 P, 모델 예측을 Q라 가정

* 최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것과 일치

* KL(P|Q) = (**-** cross entropy + entropy)  의 형태로 분해 가능

# 8)베이즈 통계학 맛보기
---

**조건부 확률**

P(A|B) 사건 B가 일어난 상황에서 사건 A가 발생활 확률

* 베이즈정리는 조건부확률을 이용해 정보를 갱신하는 방법

**조건부 확률의 시각화**

Confusion matrix 

||Actually Positive|Actually Negative|
|---|---|---|
|**Predicted**<br/>**Positive**|True Positive|False Positive<br/>(1종 오류)|
|**Predicted**<br/>**Negative**|False Negative<br/>(2종 오류)|True Negative|

**정밀도**

Precision, TP / (TP+FP)

**베이즈 정리를 통한 정보의 갱신**

새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률 계산 가능

**인과관계**

조건부 확률은 유용한 통계적 해석을 제공하지만 인과관계(causality)를 추론할 때 함부로 사용x

* 데이터가 많아져도 추론 불가능

* 인과관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때 필요(높은 예측 정확도)

* 인과관계를 알아내기 위해서는 중첩요인(confounding factor)의 효과를 제거해야함
ex)키가 크면 지능지수가 높은데 그건 중첩요인인 나이를 제거하지 않았을 경우임

# 9)CNN 첫걸음
---

**기존 MLP 구조**

다층신경망(MLP)은 각 뉴런들이 선형모델과 활성함수로 모두 연결된(fully connected) 구조

**Convolution 연산**

고정된 벡터인 커널(kernel)을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조

* 선형변환의 한 종류

* CNN에서의 연산은 엄밀히 convolution이 아닌 cross-correlation이라 부름

* 커널은 정의역 내에서 움직여도 변하지 않고(translation invariant) 주어진 신호에 국소적(local)으로 적용

* 데이터의 성격에 따라 사용하는 커널이 달라짐(1D, 2D, 3D...)-conv

**수학적 의미**

신호(signal)를 커널을 이용해 국소적으로 증폭 또는 감소시켜서 정보를 추출 또는 필터링

**2차원 Convolution 연산**

입력 행렬에 해당하는 데이터 내에서 커널 행렬을 x방향, y방향으로 한칸씩 이동하며 연산

* element-wise multiplication(성분곱) 연산을 통해 계산

입력 크기를 (H, W), 커널 크기를 (Kh, Kw), 출력 크기를 (Oh, Ow)라 가정

* Oh = H - Kh + 1

* Ow = W - Kw + 1

* ex) 28 x 28 입력을 3 x 3 커널로 2D - Conv 연살하면 26 x 26 행렬이 된다.

* 채널이 여러개인 경우 2차원 Convolution을 채널 개수만큼 적용(출력 시 채널은 하나로 통합)

**Convolution 연산의 역전파**

Convolution 연산은 커널이 모든 입력데이터에 대해 공통으로 적용되므로 역전파 시에도 convolution 연산이 나오게 된다.

# 10)RNN 첫걸음
---

**시퀀스 데이터**

소리, 문자열, 주가 등의 데이터를 시퀀스 데이터로 분류

* 독립동등분포(i.i.d) 가정을 위배할 수 있으므로 순서를 바꾸면 분포도 바뀜

* 조건부 확률을 통해 앞으로 발생할 데이터의 확률분포를 다룰 수 있음

* 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요함

**AR**

고정된 길이 t(tau)만큼의 시퀀스만 사용하는 경우 Autoregressive Model(자기회귀모델)이라고 부름

**latent autoregressive model**

잠재자기회귀모델, 이전 정보를 제외한 정보를 Ht라는 잠재 변수로 인코딩해서 활용하는 모델

**RNN**

잠재변수 Ht를 신경망을 통해 반복해서 사용하여 시퀀스 데이터의 패턴을 학습하는 모델

* 가장 기본적인 RNN 모형은 MLP와 유사한 모양

* RNN은 이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링

**BPTT**

Backpropagation Through Time, RNN의 역전파 방법

* 잠재변수의 연결그래프에 따라 순차적으로 계산

* 출력벡터 Ot에서 들어오는 gradient 벡터와 다음 시점 잠재변수 Ht+1에서 들어오는 gradient 벡터가 잠재변수 Ht에 전달하는 방식

**기울기 소실**

BPTT는 시퀀스 길이가 길어질수록 불안정해진다(전달되는 값이 1보다 크면 발산, 작으면 0으로 수렴해간다)

* 그 중 기울기가 0으로 줄어들면 기울기 소실이 발생한다

* 따라서 길이를 끊는 것이 필요(truncated BPTT)

* 이런 문제들 때문에 Vanila RNN은 길이가 긴 시퀀스를 처리하는데 문제가 있음

* 이를 해결하기 위해 등장한 RNN 네트워크가 LSTM, GRU이다

<br/>
<br/>
