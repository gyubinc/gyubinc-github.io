---
layout: post
title: "Day 4"
---

# ê³¼ì œ

**ê³¼ì œ ë‚´ìš©ì€ ë¹„ê³µê°œ**

1. Basic Math

2. Numpy

3. Pandas

4. ë²¡í„°

5. í–‰ë ¬

6. ê²½ì‚¬í•˜ê°•ë²•

7. ë”¥ëŸ¬ë‹ í•™ìŠµ ë°©ë²•

8. ë² ì´ì¦ˆ í†µê³„í•™

9. CNN

10. RNN

# 1)Vector
---
**vector**

ìˆ«ìë¥¼ ì›ì†Œë¡œ ê°€ì§€ëŠ” list ë˜ëŠ” array

* ê³µê°„ì—ì„œì˜ í•œ ì 

* ì›ì ìœ¼ë¡œë¶€í„°ì˜ ìƒëŒ€ì  ìœ„ì¹˜

* scalarë¥¼ ê³±í•˜ë©´ ë°©í–¥ ë³€í™” x, ê¸¸ì´ë§Œ ë³€í™”

**Norm**

ì›ì ì—ì„œë¶€í„°ì˜ ê±°ë¦¬ 

* L1 Norm : ì ˆëŒ€ê°’ì˜ í•©, ë§¨í•˜íƒ„ ê±°ë¦¬ $\begin{Vmatrix}L\\ \end{Vmatrix}_1$ 

* L2 Norm : ì œê³±í•©ì˜ ì œê³±ê·¼, ìœ í´ë¦¬ë“œ ê±°ë¦¬ $\begin{Vmatrix}L\\ \end{Vmatrix}_2$ 

**ë²¡í„° ì‚¬ì´ì˜ ê±°ë¦¬**

x ë²¡í„°ì™€ y ë²¡í„° ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ë²¡í„°ì˜ ëº„ì…ˆì„ ì´ìš©í•´ êµ¬í•¨

* $\begin{Vmatrix}y-x\\ \end{Vmatrix}$ 

**ë²¡í„° ì‚¬ì´ì˜ ê°ë„**

ì œ2 ì½”ì‚¬ì¸ ë²•ì¹™ì„ ì´ìš©í•´ ê³„ì‚°

>$cos \theta =  \frac{\begin{Vmatrix}x\\ \end{Vmatrix}^2_2 + \begin{Vmatrix}y\\ \end{Vmatrix}^2_2 - \begin{Vmatrix}x-y\\ \end{Vmatrix}^2_2}{2\begin{Vmatrix}x\\ \end{Vmatrix}^2_2\begin{Vmatrix}y\\ \end{Vmatrix}^2_2} = \frac{<x,y>}{\begin{Vmatrix}x\\ \end{Vmatrix}^2_2\begin{Vmatrix}y\\ \end{Vmatrix}^2_2} $

**ë‚´ì **

ì •ì‚¬ì˜(orthogonal projection)ëœ ë²¡í„°ì˜ ê¸¸ì´ì™€ ê´€ë ¨

>$ <x,y> = \displaystyle\sum_{i=1}^{d} x_i y_i = \begin{Vmatrix}x\\ \end{Vmatrix}_2 \begin{Vmatrix}y\\ \end{Vmatrix}_2cos \theta$


**shift matrix**

í–‰ë ¬ì˜ ê°’ì„ ì˜®ê¸°ê³ , ì „í™˜í•˜ê³  í•˜ê¸° ìœ„í•´ì„œ ë‹¨ìœ„í–‰ë ¬ì„ ì¡°ì‘í•  ìˆ˜ ìˆë‹¤

# 2)Matrix
---
ë²¡í„°ë¥¼ ì›ì†Œë¡œ ê°€ì§€ëŠ” 2ì°¨ì› ë°°ì—´

**ì „ì¹˜í–‰ë ¬**

í–‰ê³¼ ì—´ì˜ indexê°€ ë°”ë€ í–‰ë ¬

**ì—°ì‚°ì(operator)ë¡œ í–‰ë ¬ì„ ì´í•´**

í–‰ë ¬ì€ ë‘ ë°ì´í„°ë¥¼ ì—°ê²°ì‹œí‚¤ëŠ” ì—°ì‚°ì

* í–‰ë ¬ê³±ì„ í†µí•´ ë²¡í„°ì˜ ì°¨ì› ë³€ê²½ ê°€ëŠ¥

* íŒ¨í„´ ì¶”ì¶œ, ë°ì´í„° ì••ì¶•ë„ ê°€ëŠ¥

**ì—­í–‰ë ¬**

í–‰ë ¬ Aì˜ ì—°ì‚°ì„ ê±°ê¾¸ë¡œ ë˜ëŒë¦¬ëŠ” í–‰ë ¬

* í–‰ë ¬ì‹(determinant)ì´ 0ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ê³„ì‚° ê°€ëŠ¥

**ìœ ì‚¬ ì—­í–‰ë ¬**

ë¬´ì–´-íœë¡œì¦ˆ(Moore-Penrose) ì—­í–‰ë ¬ ($A^+$)

n $\geq m$ ì¸ ê²½ìš°,$A^+ = (A^TA)^{-1} A^T$

n $\leq m$ ì¸ ê²½ìš°$A^+ = A^T(A^TA)^{-1} $

**ì‘ìš©1**

ìœ ì‚¬ì—­í–‰ë ¬ì„ ì‚¬ìš©í•˜ë©´ ì—°ë¦½ë°©ì •ì‹ì˜ ì—¬ëŸ¬ í•´ ì¤‘ 1ê°œì˜ í•´ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ

**ì‘ìš©2**

ì„ í˜•íšŒê·€ì‹ì„ ì°¾ì„ ìˆ˜ ìˆìŒ

$X\beta = y$ ì™€ ê°™ì´ ì£¼ì–´ì§„ ë°ì´í„°ê°€ ìˆì„ ë•Œ,

$\beta = X^+y$ ë¥¼ í†µí•´ ì„ í˜•íšŒê·€ ê°€ëŠ¥ (L2-Norm ìµœì†Œí™”), 
* y ì ˆí¸ ë³„ë„ ì¶”ê°€í•´ì•¼í•¨

**í™•ì¸**

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,y)
y_test = model.predict(x_test)


X_ = np.array([np.append(x,[1]) for x in X])
beta = np.linalg.pinv(X_) @ y
y_test = np.append(x, [1]) @ beta
```

# 3)ê²½ì‚¬í•˜ê°•ë²• ê¸°ì´ˆ
---

**ë¯¸ë¶„**

ë³€í™”ìœ¨ì˜ ê·¹í•œ, ì ‘ì„ ì˜ ê¸°ìš¸ê¸°

```python
import sympy as sym
from sympy.abc import x

sym.diff(sym.poly(x**2 + 2*8*x + 3), x)

'''
result

Poly(2ğ‘¥+16,ğ‘¥,ğ‘‘ğ‘œğ‘šğ‘ğ‘–ğ‘›=â„¤)
'''
```

**ê²½ì‚¬ ìƒìŠ¹ë²•**

ë¯¸ë¶„ê°’ì„ ë”í•˜ë©´ í•¨ìˆ˜ì˜ ê·¹ëŒ€ê°’ì˜ ìœ„ì¹˜ë¡œ ì´ë™

**ê²½ì‚¬ í•˜ê°•ë²•**

ë¯¸ë¶„ê°’ì„ ë”í•˜ë©´ í•¨ìˆ˜ì˜ ê·¹ì†Œê°’ì˜ ìœ„ì¹˜ë¡œ ì´ë™

**ê²½ì‚¬í•˜ê°•ë²• : ì•Œê³ ë¦¬ì¦˜(ì¼ë³€ìˆ˜)**

* ê·¹ì†Œê°’ì— ë„ë‹¬í•˜ë©´ ë¯¸ë¶„ê°’ì˜ ì ˆëŒ€ê°’ì´ 0ì´ëœë‹¤.

* ì •í™•íˆ 0ì´ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì•„ì£¼ ì‘ì€ ê°’ epsë³´ë‹¤ ì‘ì„ ë•Œ ë©ˆì¶”ë„ë¡ ì„¤ì •

* lr(learning rate)ë¥¼ ì„¤ì •í•´ ì—…ë°ì´íŠ¸ ì†ë„ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤.

* ì¢…ë£Œì¡°ê±´(epsë³´ë‹¤ ì‘ì•„ì§€ëŠ” ìˆœê°„)ìœ¼ë¡œ ê³„ì† ì—…ë°ì´íŠ¸

**ë³€ìˆ˜ê°€ ë²¡í„°ì¼ ê²½ìš°**

* ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê²½ìš° í¸ë¯¸ë¶„(partial differentiation)ì„ ì‚¬ìš©

**gradient vector**

ê° ë³€ìˆ˜ ë³„ë¡œ í¸ë¯¸ë¶„ì„ ê³„ì‚°í•œ ê°’

nabla($\nabla$)ë¥¼ ì´ìš©í•´ í‘œí˜„

$\nabla f = (\partial x_1, \partial x_2, \partial x_3,  ...)$

* gradient vectorì— -1ì„ ê³±í•˜ë©´ ê·¹ì†Œì ìœ¼ë¡œ í˜ëŸ¬ê°€ëŠ” ë²¡í„°ê°€ ë¨

**ê²½ì‚¬í•˜ê°•ë²• : ì•Œê³ ë¦¬ì¦˜(vector)**

* abs(ì ˆëŒ€ê°’)ê°€ ì•„ë‹Œ normê°’ì´ ê°ì†Œí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜

* norm(gradient)ê°’ì´ 0ì´ ë˜ëŠ” ì‹œì ê¹Œì§€ ì´ë™

# 4)ê²½ì‚¬í•˜ê°•ë²• ì‹¬í™”
---

**ì„ í˜•íšŒê·€ë¶„ì„**

ë¬´ì–´-íœë¡œì¦ˆ ì—­í–‰ë ¬ì´ ì•„ë‹Œ ê²½ì‚¬í•˜ê°•ë²•ì„ ì´ìš©í•´ ì„ í˜•ëª¨ë¸ ì°¾ëŠ” ê²ƒì´ ì¼ë°˜ì 

**ì„ í˜•íšŒê·€ì˜ ëª©ì ì‹**

* ëª©ì ì‹ì„ ìµœì†Œí™”í•˜ëŠ” $\beta$ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©í‘œ

$||y-X \beta ||_2$

$\nabla_{\beta}||y - X \beta ||_2 = (\partial_{\beta _1}||y - X \beta ||_2, ..., \partial_{\beta _d}||y - X \beta ||_2)$

$\partial_{\beta _k}||y - X \beta ||_2= -\frac{X^T_k (y - X \beta )}{n||y - X \beta ||_2}$

```python
import numpy as np

X = np.array([[1,1], [1,2], [2,2], [2,3]])
y = np.dot(X, np.array([1,2])) + 3

beta_gd = [10.1, 15.1, -6.5]
X_ = np.array([np.append(x, [1])for x in X])
print(X_)

for t in range(5000):
    error = y - X_ @ beta_gd
    #error = error /np.linalg.norm(error)
    grad = -np.transpose(X_) @ error
    beta_gd = beta_gd - 0.01 * grad

print(beta_gd)
'''
result

[[1 1 1]
 [1 2 1]
 [2 2 1]
 [2 3 1]]
[1.00000367 1.99999949 2.99999516]
```

**ê²½ì‚¬í•˜ê°•ë²•ì˜ ìˆ˜ë ´**

* ê²½ì‚¬í•˜ê°•ë²•ì€ í•™ìŠµë¥ ê³¼ í•™ìŠµíšŸìˆ˜ë¥¼ ì ì ˆíˆ ì„ íƒí•´ì•¼ì§€ ìˆ˜ë ´

* ë³¼ë¡í•¨ìˆ˜, íŠ¹íˆ ì„ í˜•íšŒê·€ì˜ ê²½ìš° ìˆ˜ë ´ ë³´ì¥

* ë¹„ì„ í˜•íšŒê·€, ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ ëª©ì ì‹ì€ ìˆ˜ë ´ ë³´ì¥ X

**í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•**

Stochastic gradient descent(SGD)

* ë°ì´í„°ë¥¼ ì¼ë¶€ë§Œ í™œìš©í•´ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ë²•

* ì¼ë¶€ë§Œ í™œìš©í•  ê²½ìš° mini-batch SGD

* ë³¼ë¡ì´ ì•„ë‹ ê²½ìš° SGDë¥¼ í†µí•´ ê³„ì‚°

* ë”¥ëŸ¬ë‹ì˜ ê²½ìš°, ë” ìœ ìš©

* ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì—°ì‚°ìì› íš¨ìœ¨ì 

* ë§¤ ê³„ì‚°ë§ˆë‹¤ ëª©ì ì‹ ëª¨ì–‘ì´ ë°”ë€Œê²Œ ëœë‹¤.

* ê·¹ì†Œì  íƒˆì¶œ ê°€ëŠ¥
