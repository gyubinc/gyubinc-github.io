---
layout: post
title: "Day 4"
---

# 과제

**과제 내용은 비공개**

1. Basic Math

2. Numpy

3. Pandas

4. 벡터

5. 행렬

6. 경사하강법

7. 딥러닝 학습 방법

8. 베이즈 통계학

9. CNN

10. RNN

# 1)Vector
---
**vector**

숫자를 원소로 가지는 list 또는 array

* 공간에서의 한 점

* 원점으로부터의 상대적 위치

* scalar를 곱하면 방향 변화 x, 길이만 변화

**Norm**

원점에서부터의 거리 

* L1 Norm : 절대값의 합, 맨하탄 거리 $\begin{Vmatrix}L\\ \end{Vmatrix}_1$ 

* L2 Norm : 제곱합의 제곱근, 유클리드 거리 $\begin{Vmatrix}L\\ \end{Vmatrix}_2$ 

**벡터 사이의 거리**

x 벡터와 y 벡터 사이의 거리는 벡터의 뺄셈을 이용해 구함

* $\begin{Vmatrix}y-x\\ \end{Vmatrix}$ 

**벡터 사이의 각도**

제2 코사인 법칙을 이용해 계산

>$cos \theta =  \frac{\begin{Vmatrix}x\\ \end{Vmatrix}^2_2 + \begin{Vmatrix}y\\ \end{Vmatrix}^2_2 - \begin{Vmatrix}x-y\\ \end{Vmatrix}^2_2}{2\begin{Vmatrix}x\\ \end{Vmatrix}^2_2\begin{Vmatrix}y\\ \end{Vmatrix}^2_2} = \frac{<x,y>}{\begin{Vmatrix}x\\ \end{Vmatrix}^2_2\begin{Vmatrix}y\\ \end{Vmatrix}^2_2} $

**내적**

정사영(orthogonal projection)된 벡터의 길이와 관련

>$ <x,y> = \displaystyle\sum_{i=1}^{d} x_i y_i = \begin{Vmatrix}x\\ \end{Vmatrix}_2 \begin{Vmatrix}y\\ \end{Vmatrix}_2cos \theta$


**shift matrix**

행렬의 값을 옮기고, 전환하고 하기 위해서 단위행렬을 조작할 수 있다

# 2)Matrix
---
벡터를 원소로 가지는 2차원 배열

**전치행렬**

행과 열의 index가 바뀐 행렬

**연산자(operator)로 행렬을 이해**

행렬은 두 데이터를 연결시키는 연산자

* 행렬곱을 통해 벡터의 차원 변경 가능

* 패턴 추출, 데이터 압축도 가능

**역행렬**

행렬 A의 연산을 거꾸로 되돌리는 행렬

* 행렬식(determinant)이 0이 아닌 경우에만 계산 가능

**유사 역행렬**

무어-펜로즈(Moore-Penrose) 역행렬 ($A^+$)

n $\geq m$ 인 경우,$A^+ = (A^TA)^{-1} A^T$

n $\leq m$ 인 경우$A^+ = A^T(A^TA)^{-1} $

**응용1**

유사역행렬을 사용하면 연립방정식의 여러 해 중 1개의 해를 구할 수 있음

**응용2**

선형회귀식을 찾을 수 있음

$X\beta = y$ 와 같이 주어진 데이터가 있을 때,

$\beta = X^+y$ 를 통해 선형회귀 가능 (L2-Norm 최소화), 
* y 절편 별도 추가해야함

**확인**

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,y)
y_test = model.predict(x_test)


X_ = np.array([np.append(x,[1]) for x in X])
beta = np.linalg.pinv(X_) @ y
y_test = np.append(x, [1]) @ beta
```

# 3)경사하강법 기초
---

**미분**

변화율의 극한, 접선의 기울기

```python
import sympy as sym
from sympy.abc import x

sym.diff(sym.poly(x**2 + 2*8*x + 3), x)

'''
result

Poly(2𝑥+16,𝑥,𝑑𝑜𝑚𝑎𝑖𝑛=ℤ)
'''
```

**경사 상승법**

미분값을 더하면 함수의 극대값의 위치로 이동

**경사 하강법**

미분값을 더하면 함수의 극소값의 위치로 이동

**경사하강법 : 알고리즘(일변수)**

* 극소값에 도달하면 미분값의 절대값이 0이된다.

* 정확히 0이되지 않으므로 아주 작은 값 eps보다 작을 때 멈추도록 설정

* lr(learning rate)를 설정해 업데이트 속도를 조절할 수 있다.

* 종료조건(eps보다 작아지는 순간)으로 계속 업데이트

**변수가 벡터일 경우**

* 다변수 함수의 경우 편미분(partial differentiation)을 사용

**gradient vector**

각 변수 별로 편미분을 계산한 값

nabla($\nabla$)를 이용해 표현

$\nabla f = (\partial x_1, \partial x_2, \partial x_3,  ...)$

* gradient vector에 -1을 곱하면 극소점으로 흘러가는 벡터가 됨

**경사하강법 : 알고리즘(vector)**

* abs(절대값)가 아닌 norm값이 감소하는 알고리즘

* norm(gradient)값이 0이 되는 시점까지 이동

# 4)경사하강법 심화
---

**선형회귀분석**

무어-펜로즈 역행렬이 아닌 경사하강법을 이용해 선형모델 찾는 것이 일반적

**선형회귀의 목적식**

* 목적식을 최소화하는 $\beta$를 찾는 것이 목표

$||y-X \beta ||_2$

$\nabla_{\beta}||y - X \beta ||_2 = (\partial_{\beta _1}||y - X \beta ||_2, ..., \partial_{\beta _d}||y - X \beta ||_2)$

$\partial_{\beta _k}||y - X \beta ||_2= -\frac{X^T_k (y - X \beta )}{n||y - X \beta ||_2}$

```python
import numpy as np

X = np.array([[1,1], [1,2], [2,2], [2,3]])
y = np.dot(X, np.array([1,2])) + 3

beta_gd = [10.1, 15.1, -6.5]
X_ = np.array([np.append(x, [1])for x in X])
print(X_)

for t in range(5000):
    error = y - X_ @ beta_gd
    #error = error /np.linalg.norm(error)
    grad = -np.transpose(X_) @ error
    beta_gd = beta_gd - 0.01 * grad

print(beta_gd)
'''
result

[[1 1 1]
 [1 2 1]
 [2 2 1]
 [2 3 1]]
[1.00000367 1.99999949 2.99999516]
```

**경사하강법의 수렴**

* 경사하강법은 학습률과 학습횟수를 적절히 선택해야지 수렴

* 볼록함수, 특히 선형회귀의 경우 수렴 보장

* 비선형회귀, 대부분의 딥러닝 목적식은 수렴 보장 X

**확률적 경사하강법**

Stochastic gradient descent(SGD)

* 데이터를 일부만 활용해 업데이트 하는 방법

* 일부만 활용할 경우 mini-batch SGD

* 볼록이 아닐 경우 SGD를 통해 계산

* 딥러닝의 경우, 더 유용

* 모든 데이터를 사용하지 않으므로 연산자원 효율적

* 매 계산마다 목적식 모양이 바뀌게 된다.

* 극소점 탈출 가능
